{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juaano28/ProyectoFinal_TAM_CEM/blob/main/Proyecto_TAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04c4277e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Promedios diarios 2019, 2020, 2021, 2022 y 2023"
      ],
      "metadata": {
        "id": "9CDaRcXTY67G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Usa raw string para evitar problemas con las barras invertidas y agrega la extensión .csv\n",
        "df = pd.read_csv(r'/content/drive/Shareddrives/UNAL_Colab/Teoría de Aprendizaje de Máquina/P_CEM/Datasets/data_Bogota_BOGOTA.csv')\n",
        "\n",
        "# Extraer la fecha de 'fecha_medicion'\n",
        "df['fecha'] = pd.to_datetime(df['fecha_medicion'].str.split(\" \").str[0], format='%Y/%m/%d')\n",
        "\n",
        "# --- New Filtering Step: Identify probes with data in 2023 ---\n",
        "# Filter data for the year 2023\n",
        "df_2023 = df[df['fecha'].dt.year == 2023]\n",
        "\n",
        "# Get the unique probe locations that have data in 2023\n",
        "probes_with_data_in_2023 = df_2023['ubicacion_sonda'].unique()\n",
        "\n",
        "print(f\"Número de sondas con datos en 2023: {len(probes_with_data_in_2023)}\")\n",
        "print(\"Sondas con datos en 2023:\", probes_with_data_in_2023.tolist())\n",
        "\n",
        "# Filter the original DataFrame to include only these probes\n",
        "df_filtered_probes = df[df['ubicacion_sonda'].isin(probes_with_data_in_2023)].copy()\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "\n",
        "# Filter the data for the years 2019, 2020, 2021, 2022, 2023 and 2024\n",
        "# NOTE: This year filtering is now applied to df_filtered_probes\n",
        "df_filtered = df_filtered_probes[df_filtered_probes['fecha'].dt.year.isin([2019, 2020, 2021, 2022, 2023, 2024])]\n",
        "print(\"Número de registros (para sondas con data en 2023) para 2019-2024:\", len(df_filtered))\n",
        "\n",
        "\n",
        "# Crear un DataFrame con la serie completa de fechas para los años 2019, 2020, 2021, 2022, 2023 y 2024\n",
        "fechas_completas = pd.date_range(start='2019-01-01', end='2024-12-31', freq='D')\n",
        "df_fechas = pd.DataFrame({'serie': fechas_completas})\n",
        "\n",
        "# Agrupar los datos por 'fecha' y 'ubicacion_sonda' y calcular el promedio diario\n",
        "# Use df_filtered for aggregation\n",
        "df_promedio = df_filtered.groupby(['fecha', 'ubicacion_sonda'])['intensidad_campo'].mean().reset_index()\n",
        "\n",
        "# Pivotear la tabla para que cada 'ubicacion_sonda' sea una columna\n",
        "df_pivot = df_promedio.pivot(index='fecha', columns='ubicacion_sonda', values='intensidad_campo').reset_index()\n",
        "\n",
        "# Renombrar la columna 'fecha' a 'serie'\n",
        "df_pivot = df_pivot.rename(columns={'fecha': 'serie'})\n",
        "\n",
        "# Hacer un merge con el DataFrame de fechas para incluír todos los días\n",
        "df_final = pd.merge(df_fechas, df_pivot, on='serie', how='left')\n",
        "\n",
        "# Llenar los datos faltantes con \"N/A\"\n",
        "# This will fill days within 2019-2024 where a probe had no data on that specific day,\n",
        "# or probes that had data in 2023 but not in other years within the range.\n",
        "df_final = df_final.fillna(\"N/A\")\n",
        "\n",
        "# Convertir la columna 'serie' a formato de texto (YYYY-MM-DD)\n",
        "df_final['serie'] = df_final['serie'].dt.strftime('%Y-%m-%d')\n",
        "\n",
        "# Guardar en Google Drive con un nuevo nombre de archivo (optional, can overwrite or save with a new name)\n",
        "output_path = '/content/drive/Shareddrives/UNAL_Colab/Teoría de Aprendizaje de Máquina/P_CEM/Bog_promedios_diarios_2019_2024_filtered_2023_probes.csv'\n",
        "df_final.to_csv(output_path, index=False)\n",
        "print(f\"\\nPromedios diarios guardados en: {output_path}\")\n",
        "\n",
        "print(df_final.head())\n",
        "print(\"\\nShape of df_final:\", df_final.shape)"
      ],
      "metadata": {
        "id": "n3nrda8PCljf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coordenadas por sonda"
      ],
      "metadata": {
        "id": "-AiU7cpzY_hJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQAaFHbUCNuV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Usa raw string para evitar problemas con las barras invertidas y agrega la extensión .csv\n",
        "df = pd.read_csv(r'/content/drive/Shareddrives/UNAL_Colab/Teoría de Aprendizaje de Máquina/P_CEM/Datasets/data_Bogota_BOGOTA.csv')\n",
        "\n",
        "# Selección de las columnas relevantes\n",
        "df_seleccion = df[['ubicacion_sonda', 'georreferencia']]\n",
        "\n",
        "# Eliminar duplicados\n",
        "df_unico = df_seleccion.drop_duplicates(subset=['ubicacion_sonda']).copy()\n",
        "\n",
        "# Renombrar las columnas:\n",
        "df_unico = df_unico.rename(columns={\n",
        "    'ubicacion_sonda': 'posiciones',\n",
        "    'georreferencia': 'coordenadas'\n",
        "})\n",
        "\n",
        "# Guardar en Google Drive\n",
        "df_unico.to_csv('/content/drive/Shareddrives/UNAL_Colab/Teoría de Aprendizaje de Máquina/P_CEM/Zip_ubicaciones_con_coordenadas.csv', index=False)\n",
        "print(df_unico.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualización"
      ],
      "metadata": {
        "id": "6vI5VkZlN5V0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4836c944"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Cargar el archivo CSV con los promedios diarios\n",
        "df_final = pd.read_csv('/content/drive/Shareddrives/UNAL_Colab/Teoría de Aprendizaje de Máquina/P_CEM/Bogotá/Bog_promedios_diarios_2019_2024.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2172802b"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure df_final is available (from cell n3nrda8PCljf or the cell that creates it)\n",
        "# df_final should contain the data after initial daily averaging and filling with \"N/A\"\n",
        "# Ensure df_final_imputed is available (from cell 52b51111 or the cell that performs imputation)\n",
        "# df_final_imputed should contain the data after handling \"N/A\" and imputation\n",
        "\n",
        "if 'df_final' not in locals():\n",
        "    print(\"df_final not found. Please run the cell that creates df_final (e.g., n3nrda8PCljf).\")\n",
        "elif 'df_final_imputed' not in locals():\n",
        "    print(\"df_final_imputed not found. Please run the cell that performs imputation (e.g., 52b51111).\")\n",
        "else:\n",
        "    # Make a temporary copy of df_final and convert \"N/A\" to NaN to find last non-null dates\n",
        "    df_temp_dates = df_final.replace(\"N/A\", np.nan).copy()\n",
        "\n",
        "    # Ensure 'serie' is datetime\n",
        "    df_temp_dates['serie'] = pd.to_datetime(df_temp_dates['serie'])\n",
        "    df_temp_dates = df_temp_dates.set_index('serie')\n",
        "\n",
        "    # Find the last non-null index (date) for each column\n",
        "    last_valid_dates = df_temp_dates.apply(lambda x: x.last_valid_index())\n",
        "\n",
        "    # Filter probes whose last valid date is within the year 2023\n",
        "    probes_data_until_2023_series = last_valid_dates[\n",
        "        (last_valid_dates.dt.year == 2023)\n",
        "    ]\n",
        "\n",
        "    probes_data_until_2023_names = probes_data_until_2023_series.index.tolist()\n",
        "\n",
        "\n",
        "    print(\"Sondas con datos medidos (no imputados) hasta algún punto dentro del año 2023:\")\n",
        "    if not probes_data_until_2023_series.empty:\n",
        "        # Print the probes and their last valid date in 2023\n",
        "        print(probes_data_until_2023_series)\n",
        "\n",
        "        # --- Visualization Step ---\n",
        "        print(\"\\nGenerating time series plots for these probes...\")\n",
        "\n",
        "        # Filter the imputed DataFrame to include only these probes and 'serie'\n",
        "        if 'serie' in df_final_imputed.columns:\n",
        "             cols_to_plot = ['serie'] + probes_data_until_2023_names\n",
        "             df_to_plot = df_final_imputed[cols_to_plot].copy()\n",
        "        else:\n",
        "             # Handle case where 'serie' might not be in df_final_imputed (unlikely if following steps)\n",
        "             print(\"Column 'serie' not found in df_final_imputed. Cannot plot time series.\")\n",
        "             df_to_plot = None\n",
        "\n",
        "\n",
        "        if df_to_plot is not None:\n",
        "            # Ensure 'serie' is datetime for plotting\n",
        "            df_to_plot['serie'] = pd.to_datetime(df_to_plot['serie'])\n",
        "\n",
        "            # Set 'serie' as index for easier plotting\n",
        "            df_to_plot = df_to_plot.set_index('serie')\n",
        "\n",
        "            # Determine the number of probes to plot\n",
        "            num_probes_to_plot = len(probes_data_until_2023_names)\n",
        "\n",
        "            # Create subplots for each probe\n",
        "            n_cols = min(num_probes_to_plot, 4) # Use max 4 columns\n",
        "            n_rows = (num_probes_to_plot + n_cols - 1) // n_cols\n",
        "\n",
        "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 5))\n",
        "            # Handle case where there's only one subplot\n",
        "            if num_probes_to_plot == 1:\n",
        "                axes = [axes]\n",
        "            else:\n",
        "                axes = axes.flatten()\n",
        "\n",
        "            # Plot time series for each probe\n",
        "            for i, probe_name in enumerate(probes_data_until_2023_names):\n",
        "                ax = axes[i]\n",
        "                ax.plot(df_to_plot.index, df_to_plot[probe_name])\n",
        "                ax.set_title(f'Serie de Tiempo: {probe_name}')\n",
        "                ax.set_xlabel('Fecha')\n",
        "                ax.set_ylabel('Intensidad de Campo')\n",
        "                ax.grid(True)\n",
        "                plt.xticks(rotation=45, ha='right') # Rotate x-axis labels\n",
        "\n",
        "            # Hide any unused subplots\n",
        "            for j in range(num_probes_to_plot, len(axes)):\n",
        "                fig.delaxes(axes[j])\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.suptitle('Series de Tiempo para Sondas con Datos hasta 2023', y=1.02)\n",
        "            plt.show()\n",
        "\n",
        "        # --- End Visualization Step ---\n",
        "\n",
        "    else:\n",
        "        print(\"No se encontraron sondas cuya última fecha con datos medidos sea en 2023.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9d80703"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # Import seaborn for boxplots\n",
        "\n",
        "# Ensure df is available (from cell n3nrda8PCljf or the cell that loads original data)\n",
        "if 'df' not in locals():\n",
        "    print(\"df not found. Please run the cell that loads the original DataFrame (e.g., n3nrda8PCljf).\")\n",
        "else:\n",
        "    # --- FIX: Create the 'fecha' column if it doesn't exist ---\n",
        "    if 'fecha' not in df.columns:\n",
        "         df['fecha'] = pd.to_datetime(df['fecha_medicion'].str.split(\" \").str[0], format='%Y/%m/%d')\n",
        "    # -----------------------------------------------------------\n",
        "\n",
        "    # --- Identify probes with data up to 2023 (reusing logic from 2172802b) ---\n",
        "    # Create a temporary DataFrame to find last non-null dates, focusing on relevant columns\n",
        "    df_temp_dates = df[['fecha', 'ubicacion_sonda', 'intensidad_campo']].copy()\n",
        "\n",
        "    # Pivot the data to easily find the last non-null date per probe\n",
        "    df_pivot_temp = df_temp_dates.pivot_table(index='fecha', columns='ubicacion_sonda', values='intensidad_campo', aggfunc='mean') # Use mean in case of multiple measurements per day\n",
        "\n",
        "    # Find the last non-null index (date) for each column\n",
        "    last_valid_dates = df_pivot_temp.apply(lambda x: x.last_valid_index())\n",
        "\n",
        "    # Filter probes whose last valid date is within the year 2023\n",
        "    probes_data_until_2023_series = last_valid_dates[\n",
        "        (last_valid_dates.dt.year == 2023)\n",
        "    ]\n",
        "\n",
        "    probes_data_until_2023_names = probes_data_until_2023_series.index.tolist()\n",
        "\n",
        "    print(\"Sondas con datos medidos (no imputados) hasta algún punto dentro del año 2023:\")\n",
        "    if not probes_data_until_2023_series.empty:\n",
        "        print(probes_data_until_2023_series)\n",
        "    else:\n",
        "        print(\"No se encontraron sondas cuya última fecha con datos medidos sea en 2023.\")\n",
        "        probes_data_until_2023_names = [] # Ensure the list is empty if no probes found\n",
        "    # --------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    # --- Filter the original DataFrame for these probes and generate boxplots ---\n",
        "    if probes_data_until_2023_names:\n",
        "        print(\"\\nGenerando boxplots para las sondas con datos hasta 2023:\")\n",
        "        # Filter the original df to include only records for these probes\n",
        "        df_filtered_for_boxplot = df[df['ubicacion_sonda'].isin(probes_data_until_2023_names)].copy()\n",
        "\n",
        "        # Determine the number of probes to plot\n",
        "        num_probes_to_plot = len(probes_data_until_2023_names)\n",
        "\n",
        "        # Create subplots for each probe's boxplot\n",
        "        n_cols = min(num_probes_to_plot, 4) # Use max 4 columns\n",
        "        n_rows = (num_probes_to_plot + n_cols - 1) // n_cols\n",
        "\n",
        "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 5))\n",
        "        # Handle case where there's only one subplot\n",
        "        if num_probes_to_plot == 1:\n",
        "            axes = [axes]\n",
        "        else:\n",
        "            axes = axes.flatten()\n",
        "\n",
        "        # Generate boxplot for each probe\n",
        "        for i, probe_name in enumerate(probes_data_until_2023_names):\n",
        "            ax = axes[i]\n",
        "            # Filter data for the current probe\n",
        "            data_to_plot = df_filtered_for_boxplot[df_filtered_for_boxplot['ubicacion_sonda'] == probe_name]\n",
        "            sns.boxplot(y='intensidad_campo', data=data_to_plot, ax=ax)\n",
        "            ax.set_title(f'Boxplot: {probe_name}')\n",
        "            ax.set_ylabel('Intensidad de Campo')\n",
        "            ax.set_xlabel('') # No x-label needed for single boxplot per subplot\n",
        "            ax.grid(True)\n",
        "\n",
        "        # Hide any unused subplots\n",
        "        for j in range(num_probes_to_plot, len(axes)):\n",
        "            fig.delaxes(axes[j])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.suptitle('Boxplots de Intensidad de Campo para Sondas con Datos hasta 2023', y=1.02)\n",
        "        plt.show()\n",
        "\n",
        "    else:\n",
        "        print(\"\\nNo hay sondas para generar boxplots (ninguna con datos hasta 2023).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesamiento"
      ],
      "metadata": {
        "id": "xNeUh-NXR3Tb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "984558a7"
      },
      "source": [
        "## Manejo de valores faltantes\n",
        "\n",
        "### Subtask:\n",
        "Decidir cómo manejar los valores \"N/A\" (que ya hemos reemplazado por NaN) en el DataFrame `df_final`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccb58af3"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Calculate the number of missing values per column\n",
        "missing_counts = df_final.isnull().sum()\n",
        "\n",
        "# Calculate the percentage of missing values per column\n",
        "missing_percentages = (missing_counts / len(df_final)) * 100\n",
        "\n",
        "print(\"Cantidad de valores NaN por columna:\")\n",
        "print(missing_counts)\n",
        "print(\"\\nPorcentaje de valores NaN por columna:\")\n",
        "print(missing_percentages)\n",
        "\n",
        "# Define the threshold for missing values percentage\n",
        "threshold = 60\n",
        "\n",
        "# Identify columns to drop\n",
        "columns_to_drop = missing_percentages[missing_percentages > threshold].index.tolist()\n",
        "\n",
        "if columns_to_drop:\n",
        "    print(f\"\\nColumnas a eliminar (más del {threshold}% de valores faltantes):\")\n",
        "    print(columns_to_drop)\n",
        "\n",
        "    # Drop the columns from the DataFrame\n",
        "    df_final = df_final.drop(columns=columns_to_drop)\n",
        "    print(\"\\nColumnas restantes en el DataFrame:\")\n",
        "    print(df_final.columns.tolist())\n",
        "else:\n",
        "    print(f\"\\nNo hay columnas con más del {threshold}% de valores faltantes.\")\n",
        "\n",
        "# Print the number of missing values again after dropping columns\n",
        "print(\"\\nCantidad de valores NaN por columna después de eliminar columnas:\")\n",
        "print(df_final.isnull().sum())\n",
        "\n",
        "# --- Diagnostic Print: Dtypes after dropping columns ---\n",
        "print(\"\\nDtypes of df_final after dropping columns:\")\n",
        "print(df_final.dtypes)\n",
        "# -----------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52b51111"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Implementar imputación con la mediana para los valores faltantes\n",
        "# Operando sobre el DataFrame df_final (que es la salida de la celda anterior y podría tener columnas eliminadas)\n",
        "\n",
        "# Make a copy\n",
        "df_final_imputed = df_final.copy()\n",
        "\n",
        "# Convert \"N/A\" to NaN\n",
        "df_final_imputed = df_final_imputed.replace(\"N/A\", np.nan)\n",
        "\n",
        "# Separate the 'serie' column from the numeric columns\n",
        "if 'serie' in df_final_imputed.columns:\n",
        "    df_serie = df_final_imputed['serie'].copy() # Use copy to avoid SettingWithCopyWarning\n",
        "    df_numeric_only = df_final_imputed.drop(columns=['serie']).copy() # Use copy\n",
        "else:\n",
        "    # Handle case where 'serie' might have been dropped (unlikely but safe)\n",
        "    df_serie = None\n",
        "    df_numeric_only = df_final_imputed.copy() # Use copy\n",
        "\n",
        "\n",
        "# --- Ensure numeric columns are purely numeric ---\n",
        "# Convert all columns in df_numeric_only to numeric, coercing errors\n",
        "for col in df_numeric_only.columns:\n",
        "    df_numeric_only[col] = pd.to_numeric(df_numeric_only[col], errors='coerce')\n",
        "# -------------------------------------------------\n",
        "\n",
        "\n",
        "# --- Simplified Median Imputation on Numeric Columns ---\n",
        "\n",
        "# Calculate the median for the numeric columns (NaNs resulting from coerce will be ignored)\n",
        "# Use .select_dtypes(include=np.number) just in case, though all should be numeric now\n",
        "median_values = df_numeric_only.select_dtypes(include=np.number).median()\n",
        "\n",
        "# Fill NaN values in the numeric DataFrame using the calculated medians\n",
        "# Pandas aligns by column names automatically.\n",
        "df_numeric_only = df_numeric_only.fillna(median_values)\n",
        "\n",
        "# --- End Simplified Median Imputation ---\n",
        "\n",
        "\n",
        "# Combine the imputed numeric columns and the 'serie' column back\n",
        "if df_serie is not None:\n",
        "    # Reset index of both dataframes before concatenating to avoid index alignment issues\n",
        "    df_serie_reset = df_serie.reset_index(drop=True)\n",
        "    df_numeric_only_reset = df_numeric_only.reset_index(drop=True)\n",
        "\n",
        "    # Create the final DataFrame by joining the 'serie' column back\n",
        "    # Ensure alignment by index (which should be default after reset_index)\n",
        "    df_final_imputed = pd.concat([df_serie_reset, df_numeric_only_reset], axis=1)\n",
        "else:\n",
        "    # If 'serie' was not present, the imputed numeric_only is the final DataFrame\n",
        "    df_final_imputed = df_numeric_only\n",
        "\n",
        "\n",
        "# Verificar si quedan valores NaN después de la imputación con la mediana\n",
        "print(\"\\nCantidad de valores NaN por columna después de la imputación con la mediana:\")\n",
        "print(df_final_imputed.isnull().sum())\n",
        "\n",
        "# Display the first few rows of the imputed DataFrame\n",
        "print(\"\\nPrimeras filas del DataFrame imputado con la mediana:\")\n",
        "display(df_final_imputed.head())\n",
        "\n",
        "# NOTE: The subsequent cell (3c8b12c3) currently performs ffill and bfill.\n",
        "# If you intend to use only median imputation, you might skip or modify that cell."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c8b12c3"
      },
      "source": [
        "# Aplicar forward fill y luego backward fill para los NaN restantes\n",
        "df_final_imputed = df_final_imputed.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "# Asegurarse de que todas las columnas numéricas sean de tipo float\n",
        "for column in df_final_imputed.columns:\n",
        "    if column != 'serie':\n",
        "        df_final_imputed[column] = df_final_imputed[column].astype(float)\n",
        "\n",
        "\n",
        "# Verificar si quedan valores NaN después del ffill y bfill\n",
        "print(\"\\nCantidad de valores NaN por columna después de ffill y bfill:\")\n",
        "print(df_final_imputed.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3b0a4dd"
      },
      "source": [
        "## Normalización o escalado de datos\n",
        "\n",
        "### Subtask:\n",
        "Escalar los valores de intensidad de campo en el DataFrame `df_final_interpolated` a un rango específico.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "418f686c"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Select the numerical columns for scaling, excluding the 'serie' column\n",
        "numerical_cols = df_final_imputed.columns.drop('serie')\n",
        "df_to_scale = df_final_imputed[numerical_cols]\n",
        "\n",
        "# Instantiate MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "# Fit the scaler to the data and transform the data\n",
        "scaled_data = scaler.fit_transform(df_to_scale)\n",
        "\n",
        "# Create a new DataFrame with the scaled data\n",
        "df_scaled = pd.DataFrame(scaled_data, columns=numerical_cols)\n",
        "\n",
        "# Add the 'serie' column back to the scaled DataFrame\n",
        "df_scaled['serie'] = df_final_imputed['serie']\n",
        "\n",
        "# Ensure all numerical columns in df_scaled are float\n",
        "for column in df_scaled.columns:\n",
        "    if column != 'serie':\n",
        "        df_scaled[column] = df_scaled[column].astype(float)\n",
        "\n",
        "# Display the first few rows of the scaled DataFrame\n",
        "display(df_scaled.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2be7f2e5"
      },
      "source": [
        "## Creación de secuencias para lstm\n",
        "\n",
        "### Subtask:\n",
        "Crear pares de entrada-salida en formato de secuencia para el modelo LSTM utilizando el DataFrame escalado `df_scaled`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHPRKJMJS_Ym"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def create_sequences(data, n_steps):\n",
        "    X, y = [], []\n",
        "    # data should only contain the numerical columns at this point\n",
        "\n",
        "    for i in range(len(data) - n_steps):\n",
        "        # Extract the input sequence (features)\n",
        "        seq_x = data.iloc[i:(i + n_steps), :].values.astype(np.float32)\n",
        "        # Extract the output value(s) (targets)\n",
        "        seq_y = data.iloc[i + n_steps, :].values.astype(np.float32)\n",
        "\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "\n",
        "    # Convert the list of arrays to a single numpy array with specified dtype\n",
        "    # Ensure resulting arrays are float32\n",
        "    X_array = np.array(X, dtype=np.float32)\n",
        "    y_array = np.array(y, dtype=np.float32)\n",
        "\n",
        "    # --- Diagnostic Prints ---\n",
        "    # print(\"Dtypes of df_scaled_numerical before creating sequences:\")\n",
        "    # print(df_scaled_numerical.dtypes)\n",
        "    # -------------------------\n",
        "\n",
        "    # --- Diagnostic Prints ---\n",
        "    print(\"\\nDtype of resulting X array after create_sequences:\", X_array.dtype)\n",
        "    print(\"Dtype of resulting y array after create_sequences:\", y_array.dtype)\n",
        "    # -------------------------\n",
        "\n",
        "\n",
        "    return X_array, y_array\n",
        "\n",
        "# Define the number of time steps\n",
        "n_steps = 60\n",
        "\n",
        "# Select only the numerical columns from the scaled DataFrame\n",
        "df_scaled_numerical = df_scaled.drop(columns=['serie']).copy()\n",
        "\n",
        "# Create sequences and targets using only the scaled numerical data\n",
        "X, y = create_sequences(df_scaled_numerical, n_steps)\n",
        "\n",
        "\n",
        "print(\"\\nShape of input sequences (X):\", X.shape)\n",
        "print(\"Shape of output targets (y):\", y.shape)\n",
        "\n",
        "# --- Explicitly ensure X and y are float32 before splitting ---\n",
        "X = X.astype(np.float32)\n",
        "y = y.astype(np.float32)\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Split the data into training and a temporary set (for validation and test)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Split the temporary set into validation and test sets\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Print the shapes and dtypes of the resulting sets\n",
        "print(\"\\nShape of X_train:\", X_train.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of X_val:\", X_val.shape)\n",
        "print(\"Shape of y_val:\", y_val.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)\n",
        "\n",
        "print(\"\\nDtypes of training, validation, and test sets after splitting:\")\n",
        "print(\"X_train dtype:\", X_train.dtype)\n",
        "print(\"y_train dtype:\", y_train.dtype)\n",
        "print(\"X_val dtype:\", X_val.dtype)\n",
        "print(\"y_val dtype:\", y_val.dtype)\n",
        "print(\"X_test dtype:\", X_test.dtype)\n",
        "print(\"y_test dtype:\", y_test.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a223644"
      },
      "source": [
        "## División de datos\n",
        "\n",
        "### Subtask:\n",
        "Dividir el conjunto de datos en conjuntos de entrenamiento, validación y prueba.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae2b1844"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and a temporary set (for validation and test)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Split the temporary set into validation and test sets\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Print the shapes of the resulting sets\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of X_val:\", X_val.shape)\n",
        "print(\"Shape of y_val:\", y_val.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be098a64"
      },
      "source": [
        "## Reestructuración para lstm\n",
        "\n",
        "### Subtask:\n",
        "Asegurar que los datos tengan la forma correcta para la entrada del modelo LSTM (generalmente 3D: [muestras, pasos de tiempo, características]).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd70f4b4"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ensure df_scaled is available (it was created in cell 418f686c)\n",
        "# If you restarted the notebook, you might need to re-run the preprocessing cells.\n",
        "\n",
        "# Define the path where you want to save the CSV file in Google Drive\n",
        "output_path = '/content/drive/Shareddrives/UNAL_Colab/Teoría de Aprendizaje de Máquina/P_CEM/Bog_datos_preprocesados_final.csv'\n",
        "\n",
        "# Save the df_scaled DataFrame to a CSV file\n",
        "# Set index=False to avoid writing the DataFrame index as a column\n",
        "df_scaled.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Base de datos preprocesada final guardada en: {output_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b686884"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   Initial data contained a significant number of missing values (NaNs) across almost all sensor columns, ranging from approximately 426 to 1040 per column.\n",
        "*   A two-step imputation process involving linear interpolation followed by forward and backward fill successfully handled all missing values.\n",
        "*   The numerical intensity field values were scaled to a range of (0, 1) using `MinMaxScaler`.\n",
        "*   Input sequences ($X$) and output targets ($y$) were created with a sequence length of 60 time steps. The resulting shapes are $X$: $(1036, 60, 14)$ and $y$: $(1036, 14)$, indicating 1036 samples, each with 60 time steps and 14 features, and corresponding targets for the 14 sensors.\n",
        "*   The data was split into training ($X\\_train$, $y\\_train$), validation ($X\\_val$, $y\\_val$), and test ($X\\_test$, $y\\_test$) sets with shapes of $(725, 60, 14)$, $(155, 60, 14)$, and $(156, 60, 14)$ respectively for the input sequences. The corresponding output shapes are $(725, 14)$, $(155, 14)$, and $(156, 14)$.\n",
        "*   The input data arrays are already in the required 3D format ([samples, time steps, features]) for LSTM models.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The preprocessed data is now ready for training an LSTM model for forecasting intensity fields.\n",
        "*   Consider exploring different sequence lengths or data scaling methods in future iterations to optimize model performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f934bc8c"
      },
      "source": [
        "## Definición y entrenamiento del modelo LSTM\n",
        "\n",
        "### Subtask:\n",
        "Definir la arquitectura del modelo LSTM, compilarlo y entrenarlo utilizando los datos preprocesados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e404a3e5"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "import numpy as np\n",
        "\n",
        "# NOTE: Ensure the preceding preprocessing cells (loading data, handling missing values,\n",
        "# scaling, and creating sequences/splitting data) have been executed to load the latest\n",
        "# preprocessed data into X_train, y_train, X_val, y_val.\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "# Manual Refinement Step 1: Adjusting LSTM units from 50 to 75\n",
        "# Manual Refinement Step 2: Add a second LSTM layer\n",
        "model.add(LSTM(units=75, activation='relu', return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(Dropout(0.3)) # Manual Refinement Step 4: Adjust dropout rate\n",
        "model.add(LSTM(units=75, activation='relu')) # Second LSTM layer, no return_sequences=True\n",
        "model.add(Dropout(0.3)) # Manual Refinement Step 4: Adjust dropout rate\n",
        "model.add(Dense(units=y_train.shape[1])) # Output layer with the number of target features\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Summarize the model\n",
        "model.summary()\n",
        "\n",
        "# Manual Refinement Step 3: Modify training parameters (epochs and batch size)\n",
        "print(\"\\nTraining with adjusted parameters (epochs=100, batch_size=64)...\")\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_val, y_val), verbose=1)\n",
        "\n",
        "print(\"\\nTraining complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "013c71ae"
      },
      "source": [
        "## Evaluación del modelo\n",
        "\n",
        "### Subtask:\n",
        "Evaluar el rendimiento del modelo entrenado utilizando el conjunto de prueba."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b75bd42d"
      },
      "source": [
        "# --- Diagnostic Prints ---\n",
        "print(\"\\nDtypes of test data before model.evaluate:\")\n",
        "print(\"X_test dtype:\", X_test.dtype)\n",
        "print(\"y_test dtype:\", y_test.dtype)\n",
        "# -------------------------\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test set loss: {loss}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e9c8e1a"
      },
      "source": [
        "## Visualización del historial de entrenamiento\n",
        "\n",
        "### Subtask:\n",
        "Visualizar la pérdida de entrenamiento y validación a lo largo de las épocas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4037e0ba"
      },
      "source": [
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss during Training')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4de0ab68"
      },
      "source": [
        "## Visualización de Predicciones vs. Valores Reales\n",
        "\n",
        "### Subtask:\n",
        "Generar predicciones utilizando el modelo entrenado en el conjunto de prueba y compararlas visualmente con los valores reales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6b40159"
      },
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Shape of predictions (y_pred):\", y_pred.shape)\n",
        "print(\"Shape of actual values (y_test):\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4922023f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the column names for the probes from the numerical scaled DataFrame\n",
        "probe_names = df_scaled_numerical.columns.tolist()\n",
        "\n",
        "# Determine the number of probes\n",
        "num_probes = len(probe_names)\n",
        "\n",
        "# Create a figure and a set of subplots\n",
        "# Adjust the layout (nrows, ncols) and figsize based on the number of probes\n",
        "# We'll try a 4x4 grid as an example, adjust as needed\n",
        "n_cols = 4\n",
        "n_rows = (num_probes + n_cols - 1) // n_cols # Calculate number of rows needed\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 4))\n",
        "axes = axes.flatten() # Flatten the 2D array of axes for easy iteration\n",
        "\n",
        "# Iterate through each probe and create a subplot\n",
        "for i in range(num_probes):\n",
        "    ax = axes[i]\n",
        "    probe_name = probe_names[i]\n",
        "\n",
        "    # Plot actual values and predictions for the current probe\n",
        "    ax.plot(y_test[:, i], label='Valores Reales')\n",
        "    ax.plot(y_pred[:, i], label='Predicciones')\n",
        "\n",
        "    ax.set_title(f'Sonda: {probe_name}')\n",
        "    ax.set_xlabel('Paso de Tiempo (en conjunto de prueba)')\n",
        "    ax.set_ylabel('Intensidad de Campo (Escalada)')\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "\n",
        "# Hide any unused subplots\n",
        "for j in range(num_probes, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout() # Adjust layout to prevent titles/labels overlapping\n",
        "plt.suptitle('Predicciones vs. Valores Reales por Sonda (Conjunto de Prueba)', y=1.02) # Add a main title\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8da14260"
      },
      "source": [
        "## Métricas de Error Específicas\n",
        "\n",
        "### Subtask:\n",
        "Calcular métricas de error (RMSE, MAE, MAPE) para evaluar el rendimiento del modelo en el conjunto de prueba."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "471d53c9"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f'Root Mean Squared Error (RMSE) en el conjunto de prueba: {rmse}')\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f'Mean Absolute Error (MAE) en el conjunto de prueba: {mae}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5954acf6"
      },
      "source": [
        "# Validación cruzada\n",
        "Implementar validación cruzada para series de tiempo en el modelo LSTM utilizando los datos preprocesados, mostrando el código y explicando los resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01c76be9"
      },
      "source": [
        "## Preparar los datos para validación cruzada\n",
        "\n",
        "### Subtask:\n",
        "Asegurarse de que los datos estén ordenados cronológicamente y listos para ser divididos en pliegues de entrenamiento y validación secuenciales.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ada300d5"
      },
      "source": [
        "# Ensure the DataFrame is sorted by date\n",
        "df_scaled = df_scaled.sort_values(by='serie').reset_index(drop=True)\n",
        "\n",
        "# Separate features (X) and targets (y)\n",
        "# Exclude the 'serie' column from features\n",
        "features = df_scaled.drop(columns=['serie'])\n",
        "targets = df_scaled.drop(columns=['serie']) # For multi-output LSTM, targets are the same features shifted\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "X_sorted = features.values\n",
        "y_sorted = targets.values\n",
        "\n",
        "# Print shapes to verify\n",
        "print(\"Shape of sorted features (X_sorted):\", X_sorted.shape)\n",
        "print(\"Shape of sorted targets (y_sorted):\", y_sorted.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "624c4eec"
      },
      "source": [
        "# Define the number of time steps (using the same n_steps as before)\n",
        "n_steps = 60\n",
        "\n",
        "# Create sequences and targets using the sorted data\n",
        "# X will be the input sequences (n_steps) and y will be the next step (1)\n",
        "X_cv, y_cv = [], []\n",
        "\n",
        "for i in range(len(X_sorted) - n_steps):\n",
        "    # Input sequence: n_steps data points for all features\n",
        "    seq_x = X_sorted[i:(i + n_steps), :]\n",
        "    # Output target: the next data point for all features\n",
        "    seq_y = y_sorted[i + n_steps, :]\n",
        "\n",
        "    X_cv.append(seq_x)\n",
        "    y_cv.append(seq_y)\n",
        "\n",
        "# Convert the list of arrays to a single numpy array\n",
        "X_cv = np.array(X_cv)\n",
        "y_cv = np.array(y_cv)\n",
        "\n",
        "# Print the shapes to verify\n",
        "print(\"Shape of input sequences for CV (X_cv):\", X_cv.shape)\n",
        "print(\"Shape of output targets for CV (y_cv):\", y_cv.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d966e7c"
      },
      "source": [
        "## Definir la estrategia de validación cruzada\n",
        "\n",
        "### Subtask:\n",
        "Configurar un iterador de validación cruzada para series de tiempo (como `TimeSeriesSplit` de scikit-learn) que defina cómo se dividirán los datos en pliegues.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53e229d3"
      },
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "# Define the number of splits for TimeSeriesSplit\n",
        "# The number of splits determines how many train/test splits are created.\n",
        "# With n_splits=5, you will have 5 different splits.\n",
        "# The test set size for each split is approximately total_samples / (n_splits + 1)\n",
        "# Given our total number of sequences is 1036, n_splits=5 seems reasonable,\n",
        "# resulting in test sets of around 1036 / 6 = ~172 samples.\n",
        "n_splits = 5\n",
        "\n",
        "# Instantiate TimeSeriesSplit\n",
        "# TimeSeriesSplit provides train/test indices to split time series data samples\n",
        "# that are observed at fixed time intervals, in train/test sets.\n",
        "# In each split, the training set is always before the test set.\n",
        "# The test sets are successive samples that are supersets of the previous test sets.\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "print(f\"TimeSeriesSplit configured with {n_splits} splits.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76eacc84"
      },
      "source": [
        "## Implementar el bucle de validación cruzada\n",
        "\n",
        "### Subtask:\n",
        "Iterar a través de los pliegues definidos por la estrategia. En cada iteración: Dividir los datos en conjuntos de entrenamiento y validación según el pliegue actual. Crear una nueva instancia del modelo LSTM (es importante entrenar un modelo nuevo en cada pliegue para evitar la fuga de información). Entrenar el modelo en el conjunto de entrenamiento del pliegue actual. Evaluar el modelo entrenado en el conjunto de validación del pliegue actual, calculando métricas como MSE o RMSE. Almacenar las métricas de evaluación para cada pliegue.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b16be66"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "import numpy as np # numpy is already imported in previous cells\n",
        "\n",
        "# Initialize a list to store evaluation results\n",
        "cv_scores = []\n",
        "\n",
        "# Loop through the splits defined by TimeSeriesSplit\n",
        "for fold, (train_index, val_index) in enumerate(tscv.split(X_cv, y_cv)):\n",
        "    print(f\"--- Processing Fold {fold + 1}/{n_splits} ---\")\n",
        "\n",
        "    # Split data into training and validation sets for the current fold\n",
        "    X_train_fold, X_val_fold = X_cv[train_index], X_cv[val_index]\n",
        "    y_train_fold, y_val_fold = y_cv[train_index], y_cv[val_index]\n",
        "\n",
        "    # --- Diagnostic Prints ---\n",
        "    print(f\"Shape of X_train_fold: {X_train_fold.shape}\")\n",
        "    print(f\"Shape of y_train_fold: {y_train_fold.shape}\")\n",
        "    print(f\"Shape of X_val_fold: {X_val_fold.shape}\")\n",
        "    print(f\"Shape of y_val_fold: {y_val_fold.shape}\")\n",
        "    # -------------------------\n",
        "\n",
        "    # Create a new instance of the LSTM model for each fold\n",
        "    model = Sequential()\n",
        "    # Using the shapes determined during the initial model definition\n",
        "    model.add(LSTM(units=50, activation='relu', input_shape=(X_train_fold.shape[1], X_train_fold.shape[2])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units=y_train_fold.shape[1]))\n",
        "\n",
        "    # Compile the new model instance\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Train the model on the training data for the current fold\n",
        "    # verbose=0 to suppress detailed output for each epoch\n",
        "    history = model.fit(X_train_fold, y_train_fold, epochs=50, batch_size=32, validation_data=(X_val_fold, y_val_fold), verbose=0)\n",
        "\n",
        "    # Evaluate the trained model on the validation data of the current fold\n",
        "    loss = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
        "\n",
        "    # Store the evaluation metric (e.g., MSE loss)\n",
        "    cv_scores.append(loss)\n",
        "\n",
        "    # Print the fold number and the loss for that fold\n",
        "    print(f\"Fold {fold + 1} Validation Loss (MSE): {loss}\")\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"\\nCross-validation MSE scores for each fold:\", cv_scores)\n",
        "print(\"Mean Cross-validation MSE:\", np.mean(cv_scores))\n",
        "print(\"Standard Deviation of Cross-validation MSE:\", np.std(cv_scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8b2d67a"
      },
      "source": [
        "## Analizar los resultados de la validación cruzada\n",
        "\n",
        "### Subtask:\n",
        "Calcular el promedio y la desviación estándar de las métricas de evaluación obtenidas en todos los pliegues para tener una estimación general del rendimiento del modelo y su variabilidad.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "932bd6d0"
      },
      "source": [
        "# Calculate the mean of the cross-validation scores\n",
        "mean_cv_score = np.mean(cv_scores)\n",
        "\n",
        "# Calculate the standard deviation of the cross-validation scores\n",
        "std_cv_score = np.std(cv_scores)\n",
        "\n",
        "# Print the mean and standard deviation\n",
        "print(f'Mean Cross-validation Score (MSE): {mean_cv_score}')\n",
        "print(f'Standard Deviation of Cross-validation Score (MSE): {std_cv_score}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b80f3294"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The data was successfully sorted chronologically and structured into sequences with a window size of 60 time steps, ready for time series cross-validation.\n",
        "*   A `TimeSeriesSplit` object was configured with 5 splits to define the sequential train/test divisions.\n",
        "*   The cross-validation loop successfully iterated through the 5 folds, training and evaluating a new LSTM model on each fold's training and validation data.\n",
        "*   The Mean Squared Error (MSE) was calculated for each fold's validation set. The individual fold MSE scores were approximately 0.000069, 0.000076, 0.000115, 0.0681, and 0.0281.\n",
        "*   The mean cross-validation MSE across the 5 folds is approximately 0.0281.\n",
        "*   The standard deviation of the cross-validation MSE is approximately 0.0269, indicating some variability in the model's performance across the different time series splits.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The relatively high standard deviation compared to the mean MSE suggests that the model's performance varies significantly depending on the specific time period used for validation. This could indicate the presence of non-stationarity or specific challenging periods in the time series data.\n",
        "*   Further investigation into the folds with higher MSE might reveal patterns or characteristics in those data segments that the current model struggles with. Techniques like analyzing the residuals or visualizing predictions for those folds could be beneficial. Consider exploring more complex LSTM architectures, hyperparameter tuning, or incorporating external features that might explain the variability in performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b701273"
      },
      "source": [
        "## Análisis de Residuos\n",
        "\n",
        "### Subtask:\n",
        "Calcular los residuos (errores de predicción) del modelo en el conjunto de prueba y analizarlos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c75e218"
      },
      "source": [
        "# Calculate the residuals\n",
        "residuals = y_test - y_pred\n",
        "\n",
        "print(\"Shape of residuals:\", residuals.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c65e420c"
      },
      "source": [
        "### Subtask:\n",
        "Visualizar la distribución de los residuos para cada sonda."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee503478"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Get the column names for the probes\n",
        "probe_names = df_scaled_numerical.columns.tolist()\n",
        "num_probes = len(probe_names)\n",
        "\n",
        "# Create subplots for the distribution of residuals for each probe\n",
        "n_cols = 4\n",
        "n_rows = (num_probes + n_cols - 1) // n_cols\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 4))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in range(num_probes):\n",
        "    ax = axes[i]\n",
        "    probe_name = probe_names[i]\n",
        "\n",
        "    # Plot a histogram or distribution plot of the residuals for the current probe\n",
        "    sns.histplot(residuals[:, i], ax=ax, kde=True)\n",
        "\n",
        "    ax.set_title(f'Distribución de Residuos: {probe_name}')\n",
        "    ax.set_xlabel('Residuo')\n",
        "    ax.set_ylabel('Frecuencia')\n",
        "    ax.grid(True)\n",
        "\n",
        "# Hide any unused subplots\n",
        "for j in range(num_probes, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Distribución de Residuos por Sonda (Conjunto de Prueba)', y=1.02)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a195413"
      },
      "source": [
        "## Realizar Pronósticos\n",
        "\n",
        "### Subtask:\n",
        "Utilizar el modelo entrenado para hacer pronósticos futuros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6028abdd"
      },
      "source": [
        "**Reasoning**:\n",
        "Prepare the last sequence of historical data as input and use the trained `final_model` to make a single-step forecast."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbfac208"
      },
      "source": [
        "# Asegúrate de tener el DataFrame df_scaled con los datos escalados y ordenados\n",
        "# Si no lo tienes cargado, ejecuta la celda donde se creó (celda 418f686c)\n",
        "# Asegúrate de que n_steps esté definido (de la celda aHPRKJMJS_Ym)\n",
        "# Asegúrate de que el modelo entrenado (variable 'model') esté disponible (de la celda e404a3e5)\n",
        "\n",
        "# Select only the numerical columns from the scaled DataFrame\n",
        "df_scaled_numerical = df_scaled.drop(columns=['serie']).copy()\n",
        "\n",
        "# Extrae la última secuencia de 'n_steps' puntos de datos\n",
        "# El modelo fue entrenado con secuencias de n_steps\n",
        "# n_steps should be defined from the sequence creation cell (aHPRKJMJS_Ym)\n",
        "# Ensure n_steps is available in the environment. If not, define it here:\n",
        "# n_steps = 60 # Example, use the actual value used for training\n",
        "\n",
        "last_sequence = df_scaled_numerical.iloc[-n_steps:].values.astype(np.float32)\n",
        "\n",
        "# The shape of the input for the model must be [samples, time steps, features]\n",
        "# We have 1 sequence, n_steps time steps and num_features features (from df_scaled_numerical)\n",
        "num_features = df_scaled_numerical.shape[1]\n",
        "# Necesitamos remodelar la última secuencia para que tenga la forma [1, n_steps, num_features]\n",
        "last_sequence_reshaped = last_sequence.reshape(1, n_steps, num_features)\n",
        "\n",
        "# Hacer la predicción para el siguiente paso de tiempo\n",
        "# Usamos el modelo entrenado (variable 'model')\n",
        "predicted_scaled = model.predict(last_sequence_reshaped)\n",
        "\n",
        "print(\"Shape of the predicted scaled value:\", predicted_scaled.shape)\n",
        "# La predicción es un array 2D con forma (1, num_features)\n",
        "print(\"Predicted scaled value for the next time step:\")\n",
        "print(predicted_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9adb2bea"
      },
      "source": [
        "## Invertir el escalado de las predicciones\n",
        "\n",
        "### Subtask:\n",
        "Convertir las predicciones escaladas de vuelta a la escala original."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "716b59a2"
      },
      "source": [
        "# Ensure the scaler object is available (it was created in cell 418f686c)\n",
        "# If you restarted the notebook, you might need to re-run the scaling cell.\n",
        "\n",
        "# The inverse_transform method expects a 2D array, so we need to reshape if it's 1D\n",
        "# Our predicted_scaled is already 2D with shape (1, num_features), which is correct.\n",
        "\n",
        "# Inverse transform the scaled predictions\n",
        "predicted_original_scale = scaler.inverse_transform(predicted_scaled)\n",
        "\n",
        "print(\"Predicted values in original scale for the next time step:\")\n",
        "# predicted_original_scale is a 2D array with shape (1, num_features), so print the first row\n",
        "print(predicted_original_scale[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "672dd0ff"
      },
      "source": [
        "# Asegúrate de que el DataFrame original 'df' esté cargado (con los datos del rango de años deseado, ej. 2019-2024).\n",
        "# Si no es así, ejecuta la celda donde lo cargaste inicialmente (celda n3nrda8PCljf o similar, ajustando la ruta si es necesario).\n",
        "\n",
        "# Calcula el rango (mínimo y máximo) de la columna 'intensidad_campo' en el DataFrame original\n",
        "if 'df' in locals():\n",
        "    min_intensity = df['intensidad_campo'].min()\n",
        "    max_intensity = df['intensidad_campo'].max()\n",
        "    print(f\"Rango de Intensidad de Campo en los datos originales (2019-2024): [{min_intensity}, {max_intensity}]\")\n",
        "\n",
        "    # Obtén los valores pronosticados en escala original (de la celda 716b59a2)\n",
        "    # Asegúrate de que predicted_original_scale esté disponible\n",
        "    if 'predicted_original_scale' in locals():\n",
        "        print(\"\\nValores pronosticados en escala original para el siguiente paso de tiempo:\")\n",
        "        print(predicted_original_scale[0])\n",
        "\n",
        "        # Interpretación básica: comparar con el rango\n",
        "        print(\"\\nInterpretación:\")\n",
        "        print(f\"El rango de intensidad de campo en los datos históricos originales (2019-2024) va de {min_intensity:.4f} a {max_intensity:.4f}.\")\n",
        "        print(f\"Los valores pronosticados para el siguiente paso de tiempo (para las {len(predicted_original_scale[0])} sondas restantes) son:\")\n",
        "        # Imprimir cada predicción individualmente para facilitar la comparación\n",
        "        # Asegúrate de que probe_names esté disponible (de la celda 4922023f o similar, que usa df_scaled_numerical)\n",
        "        if 'probe_names' in locals() and len(probe_names) == len(predicted_original_scale[0]):\n",
        "            for i, pred_value in enumerate(predicted_original_scale[0]):\n",
        "                 print(f\"- {probe_names[i]}: {pred_value:.4f}\")\n",
        "        else:\n",
        "             print(\"No se pudieron obtener los nombres de las sondas o no coinciden con las predicciones.\")\n",
        "             print(predicted_original_scale[0])\n",
        "\n",
        "\n",
        "        # Puedes comentar si las predicciones caen dentro o fuera del rango histórico\n",
        "        all_within_range = all(min_intensity <= pred <= max_intensity for pred in predicted_original_scale[0])\n",
        "\n",
        "        if all_within_range:\n",
        "            print(\"\\nTodos los valores pronosticados caen dentro del rango observado en los datos históricos (2019-2024).\")\n",
        "        else:\n",
        "            print(\"\\nAl menos uno de los valores pronosticados cae fuera del rango observado en los datos históricos (2019-2024).\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\npredicted_original_scale no está disponible. Por favor, ejecuta las celdas de pronóstico de un solo paso (cbfac208 y 716b59a2).\")\n",
        "\n",
        "else:\n",
        "    print(\"El DataFrame original 'df' no está cargado. Por favor, cárgalo (celda n3nrda8PCljf) para obtener el rango.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d042296a"
      },
      "source": [
        "# Pronósticos multi-paso\n",
        "Genera código Python para realizar pronósticos multi-paso utilizando un modelo LSTM entrenado, siguiendo los pasos del plan proporcionado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8accf681"
      },
      "source": [
        "## Preparar la secuencia de entrada inicial\n",
        "\n",
        "### Subtask:\n",
        "Obtener la última secuencia de datos históricos (`n_steps`) del conjunto de datos escalado, similar a cómo lo hicimos para el pronóstico de un solo paso.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b86628d"
      },
      "source": [
        "**Reasoning**:\n",
        "Extract the last sequence of historical data from the scaled numerical DataFrame and reshape it for multi-step forecasting input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be347062"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement a loop to generate multi-step forecasts by repeatedly predicting the next step and using it as part of the input sequence for the subsequent prediction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e291eb9"
      },
      "source": [
        "**Reasoning**:\n",
        "Inverse transform the multi-step scaled forecasts back to the original data scale using the fitted scaler.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de57708c"
      },
      "source": [
        "# Calcula el rango (mínimo y máximo) de la columna 'intensidad_campo' en el DataFrame original\n",
        "if 'df' in locals():\n",
        "    min_intensity = df['intensidad_campo'].min()\n",
        "    max_intensity = df['intensidad_campo'].max()\n",
        "    print(f\"Rango de Intensidad de Campo en los datos originales: [{min_intensity}, {max_intensity}]\")\n",
        "\n",
        "    # Obtén los valores pronosticados en escala original (de la celda 716b59a2)\n",
        "    # Asegúrate de que predicted_original_scale esté disponible\n",
        "    if 'predicted_original_scale' in locals():\n",
        "        print(\"\\nValores pronosticados en escala original para el siguiente paso de tiempo:\")\n",
        "        print(predicted_original_scale[0])\n",
        "\n",
        "        # Interpretación básica: comparar con el rango\n",
        "        print(\"\\nInterpretación:\")\n",
        "        print(f\"El rango de intensidad de campo en los datos históricos originales va de {min_intensity:.4f} a {max_intensity:.4f}.\")\n",
        "        print(\"Los valores pronosticados para el siguiente paso de tiempo son:\")\n",
        "        # Imprimir cada predicción individualmente para facilitar la comparación\n",
        "        probe_names = df_scaled_numerical.columns.tolist() # Assuming df_scaled_numerical is available from earlier steps\n",
        "        for i, pred_value in enumerate(predicted_original_scale[0]):\n",
        "             print(f\"- {probe_names[i]}: {pred_value:.4f}\")\n",
        "\n",
        "        # Puedes comentar si las predicciones caen dentro o fuera del rango histórico\n",
        "        all_within_range = all(min_intensity <= pred <= max_intensity for pred in predicted_original_scale[0])\n",
        "\n",
        "        if all_within_range:\n",
        "            print(\"\\nTodos los valores pronosticados caen dentro del rango observado en los datos históricos.\")\n",
        "        else:\n",
        "            print(\"\\nAl menos uno de los valores pronosticados cae fuera del rango observado en los datos históricos.\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\npredicted_original_scale no está disponible. Por favor, ejecuta la celda 716b59a2 para obtener los valores pronosticados en escala original.\")\n",
        "\n",
        "else:\n",
        "    print(\"El DataFrame original 'df' no está cargado. Por favor, cárgalo para obtener el rango.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f24e08bb"
      },
      "source": [
        "## Preparar la secuencia de entrada inicial\n",
        "\n",
        "### Subtask:\n",
        "Obtener la última secuencia de datos históricos (`n_steps`) del conjunto de datos escalado, similar a cómo lo hicimos para el pronóstico de un solo paso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a55f8e0f"
      },
      "source": [
        "**Reasoning**:\n",
        "Extract the last sequence of historical data from the scaled numerical DataFrame and reshape it for multi-step forecasting input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2719cdac"
      },
      "source": [
        "# Select only the numerical columns from the scaled DataFrame\n",
        "df_scaled_numerical = df_scaled.drop(columns=['serie']).copy()\n",
        "\n",
        "# Extract the last sequence of 'n_steps' data points\n",
        "# The model was trained with sequences of n_steps\n",
        "n_steps = 60 # Ensure n_steps is defined, using the same value as before\n",
        "last_sequence = df_scaled_numerical.iloc[-n_steps:].values.astype(np.float32)\n",
        "\n",
        "# The shape of the input for the model must be [samples, time steps, features]\n",
        "# For multi-step forecasting, we will feed the predicted value back as input.\n",
        "# But for the very first step of multi-step forecasting, the input is the last known sequence.\n",
        "# We need to reshape the last sequence to have the shape [1, n_steps, num_features]\n",
        "last_sequence_reshaped = last_sequence.reshape(1, n_steps, last_sequence.shape[1])\n",
        "\n",
        "print(\"Shape of the last historical sequence for multi-step forecasting:\", last_sequence_reshaped.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ad9bd25"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement a loop to generate multi-step forecasts by repeatedly predicting the next step and using it as part of the input sequence for the subsequent prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e5565fe"
      },
      "source": [
        "# Define the number of future steps to forecast\n",
        "n_forecast_steps = 180 # Changed to approximately 6 months\n",
        "\n",
        "# Initialize a list to store the multi-step forecasts\n",
        "multi_step_forecasts = []\n",
        "\n",
        "# Use the last historical sequence as the initial input for forecasting\n",
        "current_input_sequence = last_sequence_reshaped\n",
        "\n",
        "# Loop to generate forecasts for the specified number of steps\n",
        "for i in range(n_forecast_steps):\n",
        "    # Make a prediction for the next time step using the current input sequence\n",
        "    predicted_scaled_step = model.predict(current_input_sequence)\n",
        "\n",
        "    # Store the predicted step (in scaled format)\n",
        "    multi_step_forecasts.append(predicted_scaled_step[0]) # Append the prediction for this step\n",
        "\n",
        "    # Prepare the input sequence for the next prediction\n",
        "    # Remove the oldest time step from the current input sequence\n",
        "    # Add the newly predicted step to the end of the sequence\n",
        "    current_input_sequence = np.append(current_input_sequence[:, 1:, :], predicted_scaled_step.reshape(1, 1, predicted_scaled_step.shape[1]), axis=1)\n",
        "\n",
        "    # Optional: Print progress\n",
        "    if (i + 1) % 10 == 0:\n",
        "        print(f\"Generated forecast for step {i + 1}/{n_forecast_steps}. Shape of next input sequence: {current_input_sequence.shape}\")\n",
        "\n",
        "\n",
        "# Convert the list of forecasts into a NumPy array\n",
        "multi_step_forecasts_scaled = np.array(multi_step_forecasts)\n",
        "\n",
        "print(\"\\nShape of multi-step forecasts (scaled):\", multi_step_forecasts_scaled.shape)\n",
        "# The shape should be (n_forecast_steps, num_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a98b3ee6"
      },
      "source": [
        "**Reasoning**:\n",
        "Inverse transform the multi-step scaled forecasts back to the original data scale using the fitted scaler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "571237b3"
      },
      "source": [
        "# Ensure the scaler object is available (it was created in cell 418f686c)\n",
        "# If you restarted the notebook, you might need to re-run the scaling cell.\n",
        "\n",
        "# Inverse transform the multi-step scaled forecasts\n",
        "multi_step_forecasts_original_scale = scaler.inverse_transform(multi_step_forecasts_scaled)\n",
        "\n",
        "print(\"Multi-step forecasts in original scale:\")\n",
        "print(multi_step_forecasts_original_scale)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86400052"
      },
      "source": [
        "## Visualización de Pronósticos Multi-Paso\n",
        "\n",
        "### Subtask:\n",
        "Visualizar los pronósticos multi-paso junto con los datos históricos recientes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "532a4909"
      },
      "source": [
        "**Reasoning**:\n",
        "Plot the last part of the historical data and the multi-step forecasts together to visualize the predictions in context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58694d13"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np # Ensure numpy is imported for nan check\n",
        "\n",
        "# Ensure df_scaled_numerical is available (from cell 2719cdac or earlier)\n",
        "# Ensure multi_step_forecasts_original_scale is available (from cell 571237b3)\n",
        "# Ensure n_steps is defined (from cell aHPRKJMJS_Ym or earlier)\n",
        "# Ensure probe_names is available (from cell 4922023f or earlier)\n",
        "# Ensure df_final is available (from cell n3nrda8PCljf or the cell that creates it, before final imputation)\n",
        "\n",
        "\n",
        "# --- Identify probes with data up to 2023 (reusing logic) ---\n",
        "if 'df_final' not in locals():\n",
        "    print(\"df_final not found. Cannot identify probes with data up to 2023.\")\n",
        "    # If df_final is not available, default to plotting all probes (or handle error)\n",
        "    probes_to_plot = None\n",
        "else:\n",
        "    # Make a temporary copy of df_final and convert \"N/A\" to NaN to find last non-null dates\n",
        "    df_temp_dates = df_final.replace(\"N/A\", np.nan).copy()\n",
        "\n",
        "    # Ensure 'serie' is datetime\n",
        "    df_temp_dates['serie'] = pd.to_datetime(df_temp_dates['serie'])\n",
        "    df_temp_dates = df_temp_dates.set_index('serie')\n",
        "\n",
        "    # Find the last non-null index (date) for each column\n",
        "    last_valid_dates = df_temp_dates.apply(lambda x: x.last_valid_index())\n",
        "\n",
        "    # Filter probes whose last valid date is within the year 2023\n",
        "    probes_data_until_2023_series = last_valid_dates[\n",
        "        (last_valid_dates.dt.year == 2023)\n",
        "    ]\n",
        "\n",
        "    # --- User Input: Set probes_to_plot based on identified probes ---\n",
        "    # Set probes_to_plot to the list of probes identified as having data until 2023\n",
        "    probes_to_plot = probes_data_until_2023_series.index.tolist()\n",
        "\n",
        "    if not probes_to_plot:\n",
        "        print(\"No probes found with data up to 2023. Plotting all probes instead.\")\n",
        "        probes_to_plot = None # Default to plotting all if none found\n",
        "    else:\n",
        "        print(f\"Identified probes with data up to 2023: {probes_to_plot}\")\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Get the last part of the historical data in original scale for plotting context\n",
        "# Increase the historical plot period for better visibility\n",
        "plot_history_steps = n_steps * 3 # Increased from 2*n_steps\n",
        "\n",
        "# Ensure df_final_imputed is available and in original scale (from cell 3c8b12c3)\n",
        "if 'df_final_imputed' not in locals():\n",
        "    print(\"df_final_imputed not found. Please run the data loading and imputation cells.\")\n",
        "else:\n",
        "    # Select the numerical columns from the imputed original scale data\n",
        "    df_original_numerical = df_final_imputed.drop(columns=['serie']).copy()\n",
        "\n",
        "    # Get the last historical data points to plot\n",
        "    historical_data_to_plot = df_original_numerical.iloc[-plot_history_steps:]\n",
        "\n",
        "    # Create a time index for the historical data\n",
        "    historical_dates = pd.to_datetime(df_final_imputed['serie'].iloc[-plot_history_steps:])\n",
        "\n",
        "    # Create a time index for the multi-step forecasts\n",
        "    # Assuming daily frequency, the forecast starts the day after the last historical date\n",
        "    last_historical_date = pd.to_datetime(df_final_imputed['serie'].iloc[-1])\n",
        "    forecast_dates = pd.date_range(start=last_historical_date + pd.Timedelta(days=1),\n",
        "                                   periods=len(multi_step_forecasts_original_scale),\n",
        "                                   freq='D')\n",
        "\n",
        "    # --- Diagnostic Check for NaNs ---\n",
        "    print(\"Checking for NaNs in historical data to plot:\", historical_data_to_plot.isnull().sum().sum())\n",
        "    print(\"Checking for NaNs in multi_step_forecasts_original_scale:\", np.isnan(multi_step_forecasts_original_scale).sum())\n",
        "    # ---------------------------------\n",
        "\n",
        "    # Determine which probes to plot and filter the data accordingly\n",
        "    if probes_to_plot is not None and len(probes_to_plot) > 0:\n",
        "        try:\n",
        "            # Filter historical data and forecasts based on the specified probes\n",
        "            # Ensure the probes_to_plot are actually in the columns of historical_data_to_plot\n",
        "            valid_probes_to_plot = [p for p in probes_to_plot if p in historical_data_to_plot.columns]\n",
        "            if len(valid_probes_to_plot) != len(probes_to_plot):\n",
        "                 print(\"Warning: Some specified probes not found in historical data columns.\")\n",
        "                 probes_to_plot = valid_probes_to_plot # Update probes_to_plot to valid ones\n",
        "\n",
        "            historical_data_filtered = historical_data_to_plot[probes_to_plot]\n",
        "            # Get the indices of the selected probes from the original probe_names list\n",
        "            # Need to get indices from the full list of probe_names derived from df_scaled_numerical\n",
        "            if 'probe_names' in locals():\n",
        "                 probe_indices = [probe_names.index(probe) for probe in probes_to_plot]\n",
        "                 forecasts_filtered = multi_step_forecasts_original_scale[:, probe_indices]\n",
        "                 probe_names_filtered = probes_to_plot\n",
        "                 print(f\"Plotting selected probes: {probe_names_filtered}\")\n",
        "            else:\n",
        "                 print(\"Error: probe_names not found. Cannot filter forecasts correctly.\")\n",
        "                 # Default to plotting all probes if there's an error\n",
        "                 historical_data_filtered = historical_data_to_plot\n",
        "                 forecasts_filtered = multi_step_forecasts_original_scale\n",
        "                 probe_names_filtered = df_original_numerical.columns.tolist()\n",
        "                 print(\"Plotting all probes instead due to error.\")\n",
        "\n",
        "\n",
        "        except ValueError as e:\n",
        "            print(f\"Error during filtering based on probes_to_plot. Error: {e}\")\n",
        "            # Default to plotting all probes if there's an error\n",
        "            historical_data_filtered = historical_data_to_plot\n",
        "            forecasts_filtered = multi_step_forecasts_original_scale\n",
        "            probe_names_filtered = df_original_numerical.columns.tolist()\n",
        "            print(\"Plotting all probes instead due to error.\")\n",
        "    else:\n",
        "        # If probes_to_plot is None or empty, plot all probes\n",
        "        historical_data_filtered = historical_data_to_plot\n",
        "        forecasts_filtered = multi_step_forecasts_original_scale\n",
        "        probe_names_filtered = df_original_numerical.columns.tolist()\n",
        "        print(\"Plotting all probes.\")\n",
        "\n",
        "\n",
        "    # Determine the number of probes to plot\n",
        "    num_probes_to_plot = len(probe_names_filtered)\n",
        "\n",
        "    # Create subplots for each probe\n",
        "    n_cols = min(num_probes_to_plot, 4) # Use max 4 columns\n",
        "    n_rows = (num_probes_to_plot + n_cols - 1) // n_cols\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 5)) # Adjusted figsize\n",
        "    # Handle case where there's only one subplot\n",
        "    if num_probes_to_plot == 1:\n",
        "        axes = [axes]\n",
        "    else:\n",
        "        axes = axes.flatten()\n",
        "\n",
        "\n",
        "    for i in range(num_probes_to_plot):\n",
        "        ax = axes[i]\n",
        "        probe_name = probe_names_filtered[i]\n",
        "\n",
        "        # Get the data for the current probe\n",
        "        # Access data from filtered dataframes/arrays\n",
        "        historical_data_probe = historical_data_filtered.iloc[:, i]\n",
        "        forecasts_probe = forecasts_filtered[:, i]\n",
        "\n",
        "\n",
        "        # Plot historical data\n",
        "        ax.plot(historical_dates, historical_data_probe, label='Datos Históricos')\n",
        "\n",
        "        # Plot multi-step forecasts\n",
        "        ax.plot(forecast_dates, forecasts_probe, label='Pronósticos', linestyle='--')\n",
        "\n",
        "        ax.set_title(f'Pronóstico Multi-Paso: {probe_name}')\n",
        "        ax.set_xlabel('Fecha')\n",
        "        ax.set_ylabel('Intensidad de Campo (Original Scale)')\n",
        "        ax.legend()\n",
        "        ax.grid(True)\n",
        "        plt.xticks(rotation=45, ha='right') # Rotate x-axis labels for readability\n",
        "\n",
        "    # Hide any unused subplots\n",
        "    for j in range(num_probes_to_plot, len(axes)):\n",
        "        fig.delaxes(axes[j])\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.suptitle('Pronósticos Multi-Paso vs. Datos Históricos Recientes por Sonda', y=1.02)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59c8f630"
      },
      "source": [
        "## Identificar la última fecha con datos medidos por sonda\n",
        "\n",
        "### Subtask:\n",
        "Encontrar la última fecha no nula para cada columna (sonda) en el DataFrame `df_final` (antes de la imputación completa)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d8519f6"
      },
      "source": [
        "**Reasoning**:\n",
        "Identify the last non-null index (date) for each column in the df_final DataFrame to understand the extent of measured data for each probe."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dashboard"
      ],
      "metadata": {
        "id": "5xE7n6AjN_T-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize\n",
        "!pip install streamlit -q #instalación de librerías\n",
        "!pip install pyngrok\n",
        "!pip install optuna\n",
        "!pip install streamlit pandas matplotlib seaborn scikit-learn pyngrok kagglehub\n",
        "!pip install pyngrok streamlit --quiet"
      ],
      "metadata": {
        "id": "eZuj01R3hbKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe2963f0"
      },
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from io import StringIO\n",
        "import os\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# --- Page Config ---\n",
        "st.set_page_config(page_title=\"Análisis de Intensidad de Campo\", layout=\"wide\")\n",
        "st.title(\"Dashboard de Análisis de Intensidad de Campo Electromagnético\")\n",
        "\n",
        "# --- Constants and Paths ---\n",
        "MODEL_PATH = 'lstm_model.h5'\n",
        "SCALER_PATH = 'scaler.pkl'\n",
        "PROBE_NAMES_PATH = 'probe_names.pkl'\n",
        "HISTORY_PATH = 'history.pkl'\n",
        "DATA_PATH = '/content/drive/Shareddrives/UNAL_Colab/Teoría de Aprendizaje de Máquina/P_CEM/Bog_promedios_diarios_2019_2024_filtered_2023_probes.csv'\n",
        "\n",
        "# --- Utility Functions ---\n",
        "\n",
        "@st.cache_data\n",
        "def load_data(data_path):\n",
        "    st.info(\"Cargando datos de promedios diarios...\")\n",
        "    try:\n",
        "        df = pd.read_csv(data_path)\n",
        "        df['serie'] = pd.to_datetime(df['serie'])\n",
        "    except FileNotFoundError:\n",
        "        st.error(f\"Error: No se encontró el archivo en {data_path}\")\n",
        "        return None\n",
        "    st.success(\"Datos cargados exitosamente.\")\n",
        "    return df\n",
        "\n",
        "@st.cache_data\n",
        "def run_preprocessing(_df):\n",
        "    \"\"\"\n",
        "    Ejecuta todos los pasos de preprocesamiento y devuelve los dataframes y objetos necesarios.\n",
        "    Se cachea para que solo se ejecute una vez.\n",
        "    \"\"\"\n",
        "    st.info(\"Ejecutando preprocesamiento de datos...\")\n",
        "    df = _df.copy()\n",
        "    start_date = '2019-01-01'\n",
        "    end_date = '2023-11-01'\n",
        "    mask = (df['serie'] >= start_date) & (df['serie'] <= end_date)\n",
        "    df_filtered = df.loc[mask].copy()\n",
        "    df_filtered.replace(\"N/A\", np.nan, inplace=True)\n",
        "    probe_columns = [col for col in df_filtered.columns if 'Bogota' in col]\n",
        "    for col in probe_columns:\n",
        "        df_filtered[col] = pd.to_numeric(df_filtered[col], errors='coerce')\n",
        "\n",
        "    missing_percentage = df_filtered.isnull().sum() / len(df_filtered) * 100\n",
        "    cols_to_drop = missing_percentage[missing_percentage > 60].index.tolist()\n",
        "    df_probes_dropped = df_filtered.drop(columns=cols_to_drop)\n",
        "\n",
        "    df_temp_dates = df_probes_dropped.set_index('serie')\n",
        "    last_valid_dates = df_temp_dates.apply(lambda x: x.last_valid_index())\n",
        "    probes_until_2023 = last_valid_dates[last_valid_dates.dt.year == 2023].index.tolist()\n",
        "    final_probes = ['serie'] + probes_until_2023\n",
        "    df_final_selection = df_probes_dropped[final_probes]\n",
        "\n",
        "    df_imputed = df_final_selection.copy()\n",
        "    for col in probes_until_2023:\n",
        "        median_val = df_imputed[col].median()\n",
        "        df_imputed[col].fillna(median_val, inplace=True)\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    df_to_scale = df_imputed[probes_until_2023]\n",
        "    scaled_data = scaler.fit_transform(df_to_scale)\n",
        "    df_scaled = pd.DataFrame(scaled_data, columns=probes_until_2023)\n",
        "    df_scaled['serie'] = pd.to_datetime(df_imputed['serie'].values)\n",
        "\n",
        "    st.success(\"Preprocesamiento completado.\")\n",
        "    return df_imputed, df_scaled, scaler, probes_until_2023\n",
        "\n",
        "def create_sequences(data, n_steps):\n",
        "    X, y = [], []\n",
        "    numerical_data = data.values\n",
        "    for i in range(len(data) - n_steps):\n",
        "        seq_x = numerical_data[i:(i + n_steps), :]\n",
        "        seq_y = numerical_data[i + n_steps, :]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "@st.cache_resource\n",
        "def get_or_train_model(X_train, y_train, X_val, y_val, _scaler, probe_names):\n",
        "    if all(os.path.exists(p) for p in [MODEL_PATH, SCALER_PATH, PROBE_NAMES_PATH, HISTORY_PATH]):\n",
        "        st.info(\"Cargando modelo y recursos pre-entrenados...\")\n",
        "        try:\n",
        "            model = load_model(MODEL_PATH, custom_objects={'mse': MeanSquaredError()})\n",
        "            with open(SCALER_PATH, 'rb') as f:\n",
        "                scaler = pickle.load(f)\n",
        "            with open(PROBE_NAMES_PATH, 'rb') as f:\n",
        "                saved_probe_names = pickle.load(f)\n",
        "            with open(HISTORY_PATH, 'rb') as f:\n",
        "                history = pickle.load(f)\n",
        "            st.success(\"Modelo y recursos cargados desde archivos.\")\n",
        "            return model, scaler, saved_probe_names, history\n",
        "        except Exception as e:\n",
        "            st.warning(f\"No se pudieron cargar los archivos guardados ({e}). Re-entrenando el modelo...\")\n",
        "\n",
        "    st.info(\"Entrenando un nuevo modelo LSTM...\")\n",
        "    model = Sequential([\n",
        "        LSTM(units=75, activation='relu', return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "        Dropout(0.3),\n",
        "        LSTM(units=75, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(units=y_train.shape[1])\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    with st.spinner(\"El modelo se está entrenando. Esto puede tardar unos minutos...\"):\n",
        "        history_obj = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_val, y_val), verbose=0)\n",
        "        history_dict = history_obj.history\n",
        "\n",
        "    model.save(MODEL_PATH)\n",
        "    with open(SCALER_PATH, 'wb') as f:\n",
        "        pickle.dump(_scaler, f)\n",
        "    with open(PROBE_NAMES_PATH, 'wb') as f:\n",
        "        pickle.dump(probe_names, f)\n",
        "    with open(HISTORY_PATH, 'wb') as f:\n",
        "        pickle.dump(history_dict, f)\n",
        "\n",
        "    st.success(\"Modelo entrenado y guardado exitosamente.\")\n",
        "    return model, _scaler, probe_names, history_dict\n",
        "\n",
        "\n",
        "# --- Visualization Functions ---\n",
        "def plot_time_series(df, probe_name):\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    data_to_plot = df[['serie', probe_name]].dropna()\n",
        "    ax.plot(data_to_plot['serie'], data_to_plot[probe_name])\n",
        "    ax.set_title(f'Serie de Tiempo: {probe_name}')\n",
        "    ax.set_xlabel('Fecha')\n",
        "    ax.set_ylabel('Intensidad de Campo')\n",
        "    ax.grid(True)\n",
        "    st.pyplot(fig)\n",
        "\n",
        "def plot_boxplot(df, probe_name):\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    data_to_plot = df[probe_name].dropna()\n",
        "    if not data_to_plot.empty:\n",
        "        sns.boxplot(y=data_to_plot, ax=ax)\n",
        "        ax.set_title(f'Boxplot de Intensidad de Campo: {probe_name}')\n",
        "        ax.set_ylabel('Intensidad de Campo')\n",
        "        st.pyplot(fig)\n",
        "\n",
        "def plot_training_history(history):\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.plot(history['loss'], label='Pérdida de Entrenamiento')\n",
        "    ax.plot(history['val_loss'], label='Pérdida de Validación')\n",
        "    ax.set_title('Historial de Pérdida del Modelo (Curvas de Aprendizaje)')\n",
        "    ax.set_xlabel('Época')\n",
        "    ax.set_ylabel('Pérdida (MSE)')\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "    st.pyplot(fig)\n",
        "\n",
        "def plot_predictions_vs_actuals(y_true, y_pred, probe_name, probe_names):\n",
        "    try:\n",
        "        probe_index = probe_names.index(probe_name)\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "        ax.plot(y_true[:, probe_index], label='Valores Reales')\n",
        "        ax.plot(y_pred[:, probe_index], label='Predicciones del Modelo', linestyle='--')\n",
        "        ax.set_title(f'Predicciones vs. Valores Reales para: {probe_name}')\n",
        "        ax.set_xlabel('Paso de Tiempo (en conjunto de prueba)')\n",
        "        ax.set_ylabel('Intensidad de Campo (Escala Original)')\n",
        "        ax.legend()\n",
        "        ax.grid(True)\n",
        "        st.pyplot(fig)\n",
        "    except (ValueError, IndexError):\n",
        "        st.error(f\"No se pudo encontrar la sonda '{probe_name}' en la lista de sondas del modelo.\")\n",
        "\n",
        "def make_multistep_forecast(model, scaler, df_scaled, probe_names, n_forecast_steps, n_steps):\n",
        "    st.info(f\"Generando pronóstico para los próximos {n_forecast_steps} días...\")\n",
        "    numerical_data = df_scaled[probe_names].values\n",
        "    last_sequence = numerical_data[-n_steps:]\n",
        "    input_sequence = last_sequence.reshape(1, n_steps, len(probe_names))\n",
        "\n",
        "    forecasts_scaled = []\n",
        "    progress_bar = st.progress(0)\n",
        "    for i in range(n_forecast_steps):\n",
        "        pred_scaled = model.predict(input_sequence, verbose=0)\n",
        "        forecasts_scaled.append(pred_scaled[0])\n",
        "        input_sequence = np.append(input_sequence[:, 1:, :], [[pred_scaled[0]]], axis=1)\n",
        "        progress_bar.progress((i + 1) / n_forecast_steps)\n",
        "\n",
        "    forecasts_original = scaler.inverse_transform(np.array(forecasts_scaled))\n",
        "    st.success(\"Pronóstico generado.\")\n",
        "    return forecasts_original\n",
        "\n",
        "def plot_all_forecasts(historical_df, forecasts_original, probe_names, n_steps):\n",
        "    st.subheader(\"Visualización de Pronósticos por Sonda\")\n",
        "    st.write(\"Cada gráfico muestra los datos históricos recientes y el pronóstico a futuro para una sonda individual.\")\n",
        "\n",
        "    num_probes = len(probe_names)\n",
        "    n_cols = 3\n",
        "    n_rows = (num_probes + n_cols - 1) // n_cols\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 5), squeeze=False)\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, probe_name in enumerate(probe_names):\n",
        "        ax = axes[i]\n",
        "        history_to_plot = historical_df.iloc[-n_steps*2:]\n",
        "        ax.plot(history_to_plot['serie'], history_to_plot[probe_name], label='Datos Históricos')\n",
        "\n",
        "        last_historical_date = historical_df['serie'].iloc[-1]\n",
        "        forecast_dates = pd.date_range(start=last_historical_date + pd.Timedelta(days=1), periods=len(forecasts_original))\n",
        "\n",
        "        forecast_values = forecasts_original[:, i]\n",
        "        ax.plot(forecast_dates, forecast_values, label='Pronóstico', linestyle='--', color='red')\n",
        "\n",
        "        ax.set_title(f'Pronóstico para: {probe_name}')\n",
        "        ax.set_xlabel('Fecha')\n",
        "        ax.set_ylabel('Intensidad')\n",
        "        ax.legend()\n",
        "        ax.grid(True)\n",
        "        for label in ax.get_xticklabels():\n",
        "            label.set_rotation(45)\n",
        "            label.set_ha('right')\n",
        "\n",
        "    for j in range(num_probes, len(axes)):\n",
        "        fig.delaxes(axes[j])\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
        "    st.pyplot(fig)\n",
        "\n",
        "\n",
        "# --- Main App Flow ---\n",
        "if __name__ == '__main__':\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "    df_raw = load_data(DATA_PATH)\n",
        "\n",
        "    if df_raw is not None:\n",
        "        # --- Preprocessing Step (runs once and is cached) ---\n",
        "        df_imputed, df_scaled, scaler, final_probe_names = run_preprocessing(df_raw)\n",
        "\n",
        "        # Store necessary items in session state\n",
        "        st.session_state['df_imputed'] = df_imputed\n",
        "        st.session_state['df_scaled'] = df_scaled\n",
        "        st.session_state['scaler'] = scaler\n",
        "        st.session_state['final_probe_names'] = final_probe_names\n",
        "\n",
        "        # --- UI Tabs ---\n",
        "        tab1, tab2, tab3, tab4 = st.tabs([\n",
        "            \"1. Exploración\", \"2. Preprocesamiento\", \"3. Entrenamiento y Evaluación\", \"4. Pronóstico\"\n",
        "        ])\n",
        "\n",
        "        with tab1:\n",
        "            st.header(\"1. Exploración de Datos Inicial\")\n",
        "            st.write(\"Visualizaciones de los datos crudos para entender la distribución y comportamiento de cada sonda.\")\n",
        "            st.dataframe(df_raw.head())\n",
        "            all_probe_names = [col for col in df_raw.columns if 'Bogota' in col]\n",
        "            probe_to_explore = st.selectbox(\"Selecciona una Sonda para Explorar\", options=all_probe_names, key=\"explore_select\")\n",
        "            if probe_to_explore:\n",
        "                col1, col2 = st.columns(2)\n",
        "                with col1:\n",
        "                    plot_time_series(df_raw, probe_to_explore)\n",
        "                with col2:\n",
        "                    plot_boxplot(df_raw, probe_to_explore)\n",
        "\n",
        "        with tab2:\n",
        "            st.header(\"2. Preprocesamiento Detallado\")\n",
        "            st.write(\"Pasos aplicados para limpiar y preparar los datos para el modelo.\")\n",
        "            st.markdown(\"##### Datos Después de la Imputación (Escala Original)\")\n",
        "            st.dataframe(st.session_state['df_imputed'].head())\n",
        "            st.markdown(\"##### Datos Después de la Normalización (Listos para el Modelo)\")\n",
        "            st.dataframe(st.session_state['df_scaled'].head())\n",
        "\n",
        "        with tab3:\n",
        "            st.header(\"3. Entrenamiento y Evaluación del Modelo\")\n",
        "            n_steps = st.slider(\"Pasos de tiempo (días):\", 30, 90, 60, key=\"n_steps_slider_train\")\n",
        "\n",
        "            X, y = create_sequences(st.session_state.df_scaled[final_probe_names], n_steps)\n",
        "            X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "            X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "            model, _, _, history = get_or_train_model(X_train, y_train, X_val, y_val, st.session_state.scaler, final_probe_names)\n",
        "            st.session_state['model'] = model\n",
        "\n",
        "            if model:\n",
        "                st.subheader(\"Evaluación del Modelo\")\n",
        "                if st.button(\"▶️ Evaluar Modelo\"):\n",
        "                    with st.spinner(\"Evaluando...\"):\n",
        "                        y_pred_scaled = model.predict(X_test)\n",
        "                        y_pred_original = scaler.inverse_transform(y_pred_scaled)\n",
        "                        y_test_original = scaler.inverse_transform(y_test)\n",
        "                        rmse = np.sqrt(mean_squared_error(y_test_original, y_pred_original))\n",
        "                        mae = mean_absolute_error(y_test_original, y_pred_original)\n",
        "                        st.session_state.update({'rmse': rmse, 'mae': mae, 'y_pred_original': y_pred_original, 'y_test_original': y_test_original})\n",
        "                    st.success(\"Evaluación completada.\")\n",
        "\n",
        "                if 'rmse' in st.session_state:\n",
        "                    col1, col2 = st.columns(2)\n",
        "                    col1.metric(\"RMSE (Error General)\", f\"{st.session_state['rmse']:.4f} V/m\")\n",
        "                    col2.metric(\"MAE (Error Promedio)\", f\"{st.session_state['mae']:.4f} V/m\")\n",
        "\n",
        "                st.subheader(\"Curvas de Aprendizaje\")\n",
        "                if history:\n",
        "                    plot_training_history(history)\n",
        "\n",
        "                st.subheader(\"Predicciones vs. Valores Reales\")\n",
        "                probe_to_eval = st.selectbox(\"Selecciona Sonda para Comparar Predicciones\", options=final_probe_names, key=\"eval_select\")\n",
        "                if 'y_pred_original' in st.session_state:\n",
        "                    plot_predictions_vs_actuals(st.session_state['y_test_original'], st.session_state['y_pred_original'], probe_to_eval, final_probe_names)\n",
        "\n",
        "        with tab4:\n",
        "            st.header(\"4. Pronóstico a Futuro\")\n",
        "            if 'model' in st.session_state:\n",
        "                n_forecast_steps = st.sidebar.slider(\"Días a pronosticar:\", 30, 180, 90, key=\"forecast_days_slider\")\n",
        "                if st.sidebar.button(\"🚀 Generar Pronóstico\"):\n",
        "                    with st.spinner(\"Generando pronósticos...\"):\n",
        "                        forecast_results = make_multistep_forecast(st.session_state.model, st.session_state.scaler, st.session_state.df_scaled, st.session_state.final_probe_names, n_forecast_steps, st.session_state.get('n_steps', 60))\n",
        "                        st.session_state['forecast_results'] = forecast_results\n",
        "                    st.success(\"Pronóstico generado.\")\n",
        "\n",
        "                if 'forecast_results' in st.session_state:\n",
        "                    plot_all_forecasts(st.session_state.df_imputed, st.session_state.forecast_results, st.session_state.final_probe_names, st.session_state.get('n_steps', 60))\n",
        "            else:\n",
        "                st.warning(\"El modelo debe ser entrenado primero en la Pestaña 3.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3179c80b"
      },
      "source": [
        "!wget -q -O - https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 > cloudflared-linux-amd64 && chmod +x cloudflared-linux-amd64\n",
        "!streamlit run app.py &>/content/logs.txt &\n",
        "!nohup ./cloudflared-linux-amd64 tunnel --url http://localhost:8501 &\n",
        "\n",
        "import time\n",
        "import re\n",
        "\n",
        "time.sleep(10) # Increased wait time for tunnel to establish\n",
        "\n",
        "# Read the cloudflared log to find the public URL\n",
        "try:\n",
        "    with open('nohup.out', 'r') as f:\n",
        "        for line in f:\n",
        "            # Search for the trycloudflare.com URL in the log\n",
        "            match = re.search(r'https?://\\S+\\.trycloudflare\\.com', line)\n",
        "            if match:\n",
        "                url = match.group(0)\n",
        "                print(f\"✅ ¡Tu dashboard está listo! Ábrelo aquí: {url}\")\n",
        "                break\n",
        "except FileNotFoundError:\n",
        "    print(\"No se encontró el archivo de log 'nohup.out'. Revisa los logs para más detalles.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kill -9 <PID>"
      ],
      "metadata": {
        "id": "pk_UeLahhBnM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}